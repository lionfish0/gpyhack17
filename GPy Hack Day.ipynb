{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPy Hack Day\n",
    "\n",
    "To demonstrate the paramz module we're going to consider the simple ridge-regression model.\n",
    "\n",
    "\"We'll try to fit a function $f(x)$ with polynomial basis functions. Kernel ridge regression does this by forming an objective function that is a combination of a l2 loss and a penalty that is there oto avoid the weights going too large\"\n",
    "\n",
    "$\\sum_{i=1}^N (y-\\phi(x_t)^\\top w)^2 + \\lambda \\sum_{j=0}^m w_j^2$\n",
    "\n",
    "We'll have a 2nd order polynomial so number of features $m$ goes from 0 to 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the parameters in paramz?\n",
    "\n",
    "- They are **not probabilistic** (note; in GPy you can add priors, but they are still fixed determined values). Instead they are fitted, and are then fixed - i.e. they're not integrated over. (I.e. are hyperparameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    TODO: In Alan's notebook we should pull code of model to notebook. -> DONE in Max's notebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why might checkgrad fail?\n",
    "    \n",
    "There are two things that could be wrong, if checkgrad fails;\n",
    "\n",
    " - the objective function could be wrong\n",
    " - the gradients could be wrong\n",
    " \n",
    "Also you might have to adjust the step size and ensure you're not at the mode (or a maximum). \n",
    " \n",
    "Note: In paramz there is no log-likelihood, as we're not doing anything probabilistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraining things\n",
    "\n",
    "e.g.\n",
    "\n",
    "    m.weights.constrain_fixed(value=1.0)\n",
    "\n",
    "**!Tying parameters doesn't work at the moment**\n",
    "\n",
    "### How does paramz handle boundary constraints?\n",
    "\n",
    "- It uses a sigmoid\n",
    "- One issue is that it will take a long time to get to solutions very close to the boundary, as your gradient will approach zero as you get close to the boundary.\n",
    "\n",
    "    TODO: Suggest a tie kernel should be available! E.g. https://github.com/lionfish0/clustering/blob/master/Example%20using%20the%20Tie%20Kernel.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramz Framework\n",
    "\n",
    "You always inherit from a model, e.g.;\n",
    "\n",
    "    class RidgeRegressionSimple(Model):\n",
    "    \n",
    "Where Model is in the paramz module. We'll cover more complex models later.\n",
    "\n",
    "### Constructor\n",
    "\n",
    "Note: We save our values as observable arrays (ObsAr) - these are like numpy arrays but lets us cache results of functions, e.g. if you call a method with that array twice, the second time will be quick.\n",
    "\n",
    "The key part re paramz is:\n",
    "\n",
    "    weights = Param('weights', weights)\n",
    "    self.weights = weights\n",
    "    self.link_parameter(weights)\n",
    "    \n",
    "this means it knows it has gradients, constraints and where it sits in the model. Note that you can choose its name here.\n",
    "\n",
    "Side note:\n",
    "> Later you can change the name,\n",
    ">\n",
    ">    m.weights.name = 'weights_for_ridge'\n",
    ">\n",
    "> It's best to provide unique names, as it'll add extra numbers onto the names to make them unique otherwise.\n",
    "\n",
    "Another side note:\n",
    "\n",
    "> The Cacher import - lets us add a decorator which allows us to cache the results of functions;\n",
    ">\n",
    ">   @Cache_this\n",
    ">   def myfunction(X):\n",
    ">       ...\n",
    "    \n",
    "### Param vs Parameterized\n",
    "\n",
    "Nodes are `parameterized`, leaves are Param\n",
    "\n",
    "Anything that should contain parameters is inherited from `Parameterized`, anything that is a parameter is inherited from `Param`. To make the link we call the `link_parameter` with the variable.\n",
    "\n",
    "For example we might have a GP model (as the top level). This might have a parameter (such as the noise variance):\n",
    "\n",
    "Parameterized [GP Model]\n",
    "    - Parameterized [kernel]\n",
    "        - Param [lengthscale]\n",
    "        - Param [variance]\n",
    "    - Param [noise variance]\n",
    "    - Parameterized [another kernel?]\n",
    "    \n",
    "Comment: !Bit slow in deep hierarchical models; should update in different orders.\n",
    "\n",
    "When you change, e.g the lengthscale of the kernel, it'll effectively need to update the kernel and that will trigger an update of the GP Model - as this will need to know that the likelihood has to be recomputed.\n",
    "\n",
    "The Param class is inherited from numpy array, so it can be treated just as a numpy array.\n",
    "\n",
    "### parameters_changed method:\n",
    "\n",
    "Any parameters that you've linked (e.g. the weight in the regression model) the model will automatically call 'parameters_changed') - this means that we can update things like the objective, the gradients, etc.\n",
    "\n",
    "Note: The model actually calls the method `objective_function()` etc when it's optimising etc. but it's worth computing such stuff in `parameters_changed` so we can then just return it when required.\n",
    "\n",
    "### .gradient\n",
    "\n",
    "Any parameter has an attribute called `gradient` which we set here; e.g.\n",
    "    \n",
    "    self.weight.gradient[:] = self._lambda*2*self.weights\n",
    "    \n",
    "E.g. If you thought that the weights had different gradients, then you could do\n",
    "\n",
    "    self.weight[2].gradient[:] = self._lambda*2*self.weights\n",
    "    \n",
    "The `[:]` is so that we set the content of the gradient array (not try to point it somewhere else).\n",
    "\n",
    "### Summary\n",
    "\n",
    "Every time the gradient changes the `parameters_changed` gets called... which updates the gradients and objectives. We do this repeatedly until we reach the mode of the objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPy\n",
    "\n",
    "So far we've just looked at a simple `Paramz` model.\n",
    "\n",
    "Let's look at GPy.\n",
    "\n",
    "A GPy model is structured as follows;\n",
    "\n",
    "```\n",
    "                     Model\n",
    "               /    |         \\\n",
    "              /     |           \\\n",
    "             /      |             \\\n",
    " inference_method  likelihood      kernel\n",
    "                    |               /     \\\n",
    "             noise_variance  lengthscale variance\n",
    "```\n",
    "\n",
    "The likelihood and the kernel are likly to have their own parameters (e.g. noise variane in the likelihood function and the kernel could have lengthscale, etc).\n",
    "\n",
    "#### Inference_method\n",
    "\n",
    "The objective function is the negative log marginal likelihood:\n",
    "\n",
    "$log p(y|\\sigma_l^2, \\theta_k) = \\int p(y|f, \\sigma_l^2) p(f|\\theta_k) df$\n",
    "\n",
    "where there's a latent function that **is** integrated over, and some hyperparameters that are not.\n",
    "\n",
    "The two distributions (the prior and the data-likelihood) are both normal, so the marginal likelihood (proportional to the posterior) is also normal;\n",
    "\n",
    "$log\\;\\;N(y|0, K(X,X) + \\sigma^2 I)$\n",
    "\n",
    "We can differentiate this log likelihood with respect to the two (or more) (hyper)parameters.\n",
    "\n",
    "#### Likelihood method\n",
    "\n",
    "If the likelihood is not Gaussian then can't do the simple analytical solution above, e.g. it could be the bernoulli distribution. So we have to approximate the result.\n",
    "\n",
    "When you change the likelihood the inference method must be changed.\n",
    "\n",
    "### Visiting the code... gp.py\n",
    "\n",
    "Things to note:\n",
    "\n",
    "- GPy's gp.py does normalising for you [TODO maybe this needs clarifying somewhere?]\n",
    "- Y_metadata, useful for e.g. multiple output GPs, so you know which output is which\n",
    "\n",
    "The kernel and likelihood are objects inherited from parameterized (i.e. they contain parameters)\n",
    "\n",
    "inference_method is just an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning in stationary: failed to import cython module: falling back to numpy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       ".pd{\n",
       "    font-family: \"Courier New\", Courier, monospace !important;\n",
       "    width: 100%;\n",
       "    padding: 3px;\n",
       "}\n",
       "</style>\n",
       "\n",
       "<p class=pd>\n",
       "<b>Model</b>: gp<br>\n",
       "<b>Objective</b>: -1688.7145410219225<br>\n",
       "<b>Number of Parameters</b>: 3<br>\n",
       "<b>Number of Optimization Parameters</b>: 3<br>\n",
       "<b>Updates</b>: True<br>\n",
       "</p>\n",
       "<style type=\"text/css\">\n",
       ".tg  {font-family:\"Courier New\", Courier, monospace !important;padding:2px 3px;word-break:normal;border-collapse:collapse;border-spacing:0;border-color:#DCDCDC;margin:0px auto;width:100%;}\n",
       ".tg td{font-family:\"Courier New\", Courier, monospace !important;font-weight:bold;color:#444;background-color:#F7FDFA;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#DCDCDC;}\n",
       ".tg th{font-family:\"Courier New\", Courier, monospace !important;font-weight:normal;color:#fff;background-color:#26ADE4;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#DCDCDC;}\n",
       ".tg .tg-left{font-family:\"Courier New\", Courier, monospace !important;font-weight:normal;text-align:left;}\n",
       ".tg .tg-center{font-family:\"Courier New\", Courier, monospace !important;font-weight:normal;text-align:center;}\n",
       ".tg .tg-right{font-family:\"Courier New\", Courier, monospace !important;font-weight:normal;text-align:right;}\n",
       "</style>\n",
       "<table class=\"tg\"><tr><th><b>  gp.                    </b></th><th><b>value</b></th><th><b>constraints</b></th><th><b>priors</b></th></tr>\n",
       "<tr><td class=tg-left>  rbf.variance           </td><td class=tg-right>  1.0</td><td class=tg-center>    +ve    </td><td class=tg-center>      </td></tr>\n",
       "<tr><td class=tg-left>  rbf.lengthscale        </td><td class=tg-right>  1.0</td><td class=tg-center>    +ve    </td><td class=tg-center>      </td></tr>\n",
       "<tr><td class=tg-left>  Gaussian_noise.variance</td><td class=tg-right> 0.01</td><td class=tg-center>    +ve    </td><td class=tg-center>      </td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<GPy.core.gp.GP at 0x7fd9da8eacc0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import GPy\n",
    "import numpy as np\n",
    "X = np.arange(0,10,0.2)[:,None]\n",
    "Y = np.sin(X)+np.random.randn(len(X))\n",
    "\n",
    "kern = GPy.kern.RBF(1)\n",
    "lik = GPy.likelihoods.Gaussian(variance=1e-2)\n",
    "exact = GPy.inference.latent_function_inference.ExactGaussianInference()\n",
    "exact = GPy.inference.latent_function_inference.ExactGaussianInference()\n",
    "GPy.core.GP(X=X, Y=Y, kernel=kern, likelihood=lik, inference_method=exact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Bit vague on this;]\n",
    "\n",
    "Note: Inside gp.py we link the kernel and likelihood, so when the kernel changes, this propagates to the model, which updates things like the objective... etc.\n",
    "\n",
    "All the inference methods take, X, the likelihood method and the kernel, and requires [TODO: What?]\n",
    "\n",
    "The parameters_changed method is implemented, and calls the `inference` method of the inference_method we chose earlier.\n",
    "This method also calls the kernel's `update_gradients_full`.\n",
    "\n",
    "To find out more about this and writing new kernels, see http://pythonhosted.org/GPy/tuto_creating_new_kernels.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing kernels, notes\n",
    "\n",
    "## Implementing kernels\n",
    "\n",
    "I realise all we covered in the talk is at http://pythonhosted.org/GPy/tuto_creating_new_kernels.html\n",
    "\n",
    "## Kernel slicing\n",
    "\n",
    "Will probably be in the kernel documentation somewhere,\n",
    "\n",
    "`k = GPy.kern.RBF(1,active_dims=[0]) + GPy.kern.Matern52(1,active_dims=[1])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# paramz example\n",
    "\n",
    "This is Max's paramz example details; could probably be moved out to a new notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates optimising a model with paramz. GPy uses paramz to optimise hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import paramz\n",
    "import numpy as np\n",
    "\n",
    "#we'll use the rosenbrock function to demonstrate this - we want to minimise the rosenbrock function\n",
    "from scipy.optimize import rosen_der, rosen\n",
    "\n",
    "#start point\n",
    "x = np.array([4,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We make our model \"Rosen\", that inherits Model so that it includes\n",
    "#Parameterized which allows us to have a parameter that's optimised.\n",
    "class Rosen(paramz.Model):\n",
    "    \n",
    "    #the constructor calls the parent constructor, and links the one parameter to the model.\n",
    "    def __init__(self, x, name='rosen'):\n",
    "        super(Rosen,self).__init__(name=name)\n",
    "        self.x = paramz.Param('position',x)\n",
    "        self.link_parameter(self.x)\n",
    "        \n",
    "    #this gives the objective function, note that usually we'd store the objective\n",
    "    #in an instance variable, e.g. in parameters_changed (to cache it) for use later.\n",
    "    def objective_function(self):\n",
    "        return rosen(self.x)\n",
    "    \n",
    "    #this is run every time a parameter is altered.\n",
    "    def parameters_changed(self):\n",
    "        self.x.gradient = rosen_der(self.x)\n",
    "\n",
    "#create the object\n",
    "r = Rosen(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that if we change a parameter, the objective is updated automatically;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name : rosen\n",
      "Objective : 22509.0\n",
      "Number of Parameters : 2\n",
      "Number of Optimization Parameters : 2\n",
      "Updates : True\n",
      "Parameters:\n",
      "  \u001b[1mrosen.  \u001b[0;0m  |  value  |  constraints\n",
      "  \u001b[1mposition\u001b[0;0m  |   (2,)  |             \n",
      "\n",
      "Name : rosen\n",
      "Objective : 980181.0\n",
      "Number of Parameters : 2\n",
      "Number of Optimization Parameters : 2\n",
      "Updates : True\n",
      "Parameters:\n",
      "  \u001b[1mrosen.  \u001b[0;0m  |  value  |  constraints\n",
      "  \u001b[1mposition\u001b[0;0m  |   (2,)  |             \n"
     ]
    }
   ],
   "source": [
    "print(r)\n",
    "r.x[0]=10\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's optimise and check that we're in the right place [1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[1mindex\u001b[0;0m  |  rosen.position  |  constraints\n",
      "  \u001b[1m[0]  \u001b[0;0m  |      1.00000000  |             \n",
      "  \u001b[1m[1]  \u001b[0;0m  |      1.00000000  |             \n"
     ]
    }
   ],
   "source": [
    "r.optimize()\n",
    "\n",
    "print(r.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraints\n",
    "\n",
    "Easy to add;\n",
    "\n",
    "e.g.\n",
    "    `r.x.constrain_bounded(-2.0,0.5)`\n",
    "\n",
    "But, if setting one value of an array:\n",
    "**To set constraints on just one of the parameters in the x array, we need to add an extra [] to stop the parameter being returned as a simple float!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: reconstraining parameters rosen.position\n",
      "WARNING: reconstraining parameters rosen.position\n"
     ]
    }
   ],
   "source": [
    "r.x[[0]].constrain_bounded(-5,1)\n",
    "r.x[[1]].constrain_bounded(-2,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#r.x[1].constrain_bounded(-4.0,1.5)\n",
    "r.randomize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<paramz.optimization.optimization.opt_lbfgsb at 0x7fd9d60fca58>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       ".tg  {padding:2px 3px;word-break:normal;border-collapse:collapse;border-spacing:0;border-color:#DCDCDC;margin:0px auto;width:100%;}\n",
       ".tg td{font-family:\"Courier New\", Courier, monospace !important;font-weight:bold;color:#444;background-color:#F7FDFA;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#DCDCDC;}\n",
       ".tg th{font-family:\"Courier New\", Courier, monospace !important;font-weight:normal;color:#fff;background-color:#26ADE4;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#DCDCDC;}\n",
       ".tg .tg-left{font-family:\"Courier New\", Courier, monospace !important;font-weight:normal;text-align:left;}\n",
       ".tg .tg-right{font-family:\"Courier New\", Courier, monospace !important;font-weight:normal;text-align:right;}\n",
       "</style>\n",
       "<table class=\"tg\">\n",
       "\n",
       "<tr>\n",
       "  <th><b>index</b></th>\n",
       "  <th><b>rosen.position</b></th>\n",
       "  <th><b>constraints</b></th>\n",
       "</tr>\n",
       "<tr><td class=tg-left>  [0]  </td><td class=tg-right>    0.70855950</td><td class=tg-left> -5.0,1.0  </td></tr>\n",
       "<tr><td class=tg-left>  [1]  </td><td class=tg-right>    0.49999988</td><td class=tg-left> -2.0,0.5  </td></tr>"
      ],
      "text/plain": [
       "\u001b[1mrosen.position\u001b[0;0m:\n",
       "Param([ 0.7085595 ,  0.49999988])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood there is a transformation that's turning the domain of the constraint to the whole of the real numbers. So when we look at `r.optimizer_array` it **won't have the same values as the actual parameters**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.97488614,  16.86102108])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.optimizer_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate we look at the function that is applied to the above;\n",
    "\n",
    "[TODO I can't quite see how we actually get the **actual** function]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Logistic, array([0])), (Logistic, array([1]))]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(r.x.constraints.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Logistic, array([0]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(r.x.constraints.items())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta notes\n",
    "\n",
    "## Who is now responsible?\n",
    "\n",
    "Mike is worried that the project will faulter if there isn't a key (paid) go-to person for the project. Also need to decide on direction, etc.\n",
    "\n",
    "Need cash! \n",
    "\n",
    "Possibly from:\n",
    "\n",
    " - Amazon\n",
    " - EPSRC\n",
    "\n",
    "## To do\n",
    "\n",
    "- Need to improve test coverage + unit testing notebooks\n",
    "- Documentation - multiple ways in, lots of dead old notebooks\n",
    "- New features\n",
    "          - tied kernel\n",
    "\n",
    "## Documentation\n",
    "\n",
    "The stuff Tania has written;\n",
    "- adds a menu\n",
    "- adds a button to download locally\n",
    "- run on the cloud (azure service completely free)\n",
    "- tests the notebooks\n",
    "\n",
    "Key problems currently\n",
    "- too many entry points\n",
    "- old code floating about (wrong versions etc)\n",
    "\n",
    "Idea\n",
    "- use sklearn's welcome page\n",
    "\n",
    "\n",
    "\n",
    "### Plan\n",
    "\n",
    "- slack channel for documentation notebook\n",
    "- Need an outline\n",
    "   - notebook A should go there, B there... etc\n",
    "   - we need to fill this hole here\n",
    "   - need to document this feature\n",
    "   - delete that notebook\n",
    "   \n",
    "- hunt down and kill all the old notebooks\n",
    "\n",
    "- Also need to include (in the doc plan/front page):\n",
    "\n",
    "   - variational methods\n",
    "   - mean functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
